{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%run common.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from piex.explorer import S3PipelineExplorer\n",
    "ex = S3PipelineExplorer('ml-pipelines-2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id_list = ex.get_datasets()['dataset'].tolist()\n",
    "biglist = ['124_153_svhn_cropped', '31_urbansound', 'bone_image_classification', 'bone_image_collection']\n",
    "dataset_id_list = [l for l in dataset_id_list if l not in biglist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_record_path(dataset_id):\n",
    "    record_path = pathlib.Path(DATA_PATH, 'records')\n",
    "    if not record_path.exists():\n",
    "        record_path.mkdir()\n",
    "        \n",
    "    return record_path.joinpath(f'{dataset_id}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_disk_usage_compressed(dataset_id):\n",
    "    path = os.path.join(DATA_PATH, f'{dataset_id}.tar.gz')\n",
    "    return os.path.getsize(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_disk_usage_uncompressed(dataset_id):\n",
    "    start_path = os.path.join(DATA_PATH, dataset_id)\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(start_path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            # skip if it is symbolic link\n",
    "            if not os.path.islink(fp):\n",
    "                total_size += os.path.getsize(fp)\n",
    "\n",
    "    return total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_record(dataset_id, record):\n",
    "    path = get_record_path(dataset_id)\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(record, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_record(dataset_id):\n",
    "    path = get_record_path(dataset_id)\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists_record(dataset_id):\n",
    "    path = get_record_path(dataset_id)\n",
    "    return os.path.exists(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@fy.decorator\n",
    "def jsoncached(call):\n",
    "    dataset_id = call.dataset_id\n",
    "    if exists_record(dataset_id):\n",
    "        return load_record(dataset_id)\n",
    "    else:\n",
    "        record = call()\n",
    "        save_record(dataset_id, record)\n",
    "        return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@quiet\n",
    "@jsoncached\n",
    "def create_record(dataset_id):\n",
    "    dataset = load_dataset(dataset_id)\n",
    "    if dataset is not None:\n",
    "        size = getsize(dataset)\n",
    "        n = len(dataset.y)\n",
    "        m = dataset.X.shape[1]\n",
    "        classes = len(np.unique(dataset.y))\n",
    "        resources = len(dataset.context.keys())\n",
    "        del dataset\n",
    "    else:\n",
    "        size = np.nan\n",
    "        n = np.nan\n",
    "        m = np.nan\n",
    "        classes = np.nan\n",
    "        resources = np.nan\n",
    "    record = {\n",
    "        'dataset_id': dataset_id,\n",
    "        'size': size,\n",
    "        'n': n,\n",
    "        'm': m,\n",
    "        'classes': classes,\n",
    "        'resources': resources,\n",
    "    }\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/476 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "records = []\n",
    "for dataset_id in tqdm(dataset_id_list):\n",
    "    record = create_record(dataset_id)\n",
    "    records.append(record)\n",
    "records[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@quiet\n",
    "def download_big(dataset_id):\n",
    "    download_dataset(BUCKET, dataset_id, DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_id in tqdm(biglist):\n",
    "    download_big(dataset_id)\n",
    "    record = {\n",
    "        'dataset_id': dataset_id,\n",
    "        'size': np.nan,\n",
    "        'n': np.nan,\n",
    "        'm': np.nan,\n",
    "        'classes': np.nan,\n",
    "        'resources': np.nan,\n",
    "    }\n",
    "    records.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in tqdm(records):\n",
    "    dataset_id = record['dataset_id']\n",
    "    du_compressed = get_disk_usage_compressed(dataset_id)\n",
    "    du_uncompressed = get_disk_usage_uncompressed(dataset_id)\n",
    "    record['du_compressed'] = du_compressed\n",
    "    record['du_uncompressed'] = du_uncompressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(records)\n",
    "df = df[['dataset_id', 'n', 'm', 'classes', 'resources', 'du_compressed', 'du_uncompressed', 'size']]\n",
    "df = df.rename(columns={\n",
    "    'size': f'Size (memory)',\n",
    "    'du_compressed': f'Size (compressed)',\n",
    "    'du_uncompressed': f'Size (uncompressed)',\n",
    "    'n': 'Number of Examples',\n",
    "    'm': 'Dimension of X',\n",
    "    'classes': 'Number of classes',\n",
    "    'resources': 'Number of resources'\n",
    "})\n",
    "\n",
    "tmp = ex.get_datasets()\n",
    "msk = tmp['task_type'] == 'classification'\n",
    "cls_ids = tmp[msk]\n",
    "cls_ids = cls_ids['dataset'].tolist()\n",
    "df.loc[~df['dataset_id'].isin(cls_ids), 'Number of classes'] = np.nan\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = df.describe()\n",
    "summary = summary.rename(columns={'Dimension of X': 'Columns of \\\\$X\\\\$', 'Number of Examples': 'Number of examples'})\n",
    "summary = summary[['Number of examples', 'Number of classes', 'Columns of \\\\$X\\\\$', 'Number of resources', f'Size (compressed)', f'Size (uncompressed)']]\n",
    "summary = summary.T\n",
    "summary = summary[['min', '25%', '50%', '75%', 'max']]\n",
    "summary = summary.rename(columns={'25%': 'p25', '50%': 'p50', '75%': 'p75'})\n",
    "summary.loc[['Size (compressed)', 'Size (uncompressed)']] = summary.loc[['Size (compressed)', 'Size (uncompressed)']].applymap(sizeof_fmt)\n",
    "summary.to_csv('task_characteristics.csv')\n",
    "summary.to_latex('task_characterstics.tex', float_format=\"{:0.1f}\".format)\n",
    "summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
